{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Implementing the ROCK Algorithm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to categorize latitude and longitude into zones\n",
    "def categorize_zone(latitude, longitude):\n",
    "    num_lat_zones = 10\n",
    "    num_lon_zones = 10\n",
    "    lat_zone_size = (max(latitude) - min(latitude)) / num_lat_zones\n",
    "    lon_zone_size = (max(longitude) - min(longitude)) / num_lon_zones\n",
    "    zones = []\n",
    "    for lat, lon in zip(latitude, longitude):\n",
    "        lat_zone = int((lat - min(latitude)) // lat_zone_size)\n",
    "        lon_zone = int((lon - min(longitude)) // lon_zone_size)\n",
    "        zones.append((lat_zone, lon_zone))\n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle missing values and preprocess the data\n",
    "def preprocess_data(track_points_df):\n",
    "    track_points_df.dropna(inplace=True)\n",
    "    track_points_df['zone'] = categorize_zone(track_points_df['latitude'], track_points_df['longitude'])\n",
    "    return track_points_df\n",
    "\n",
    "# Function to compute Jaccard similarity coefficient for zones\n",
    "def jaccard_similarity(A, B):\n",
    "    intersection = len(set(A) & set(B))\n",
    "    union = len(set(A) | set(B))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Function to compute links matrix using Jaccard similarity\n",
    "def compute_links(data, threshold):\n",
    "    num_points = len(data)\n",
    "    links_matrix = np.zeros((num_points, num_points))\n",
    "    for i in range(num_points):\n",
    "        for j in range(i+1, num_points):\n",
    "            similarity = jaccard_similarity(data[i], data[j])\n",
    "            if similarity >= threshold:\n",
    "                links_matrix[i][j] = links_matrix[j][i] = 1\n",
    "    return links_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCK Algorithm Implementation\n",
    "def ROCK(data, threshold):\n",
    "    links_matrix = compute_links(data, threshold)\n",
    "    num_points = len(data)\n",
    "    # print(\"len is \",num_points)\n",
    "    visited = [False] * num_points\n",
    "    clusters = []\n",
    "\n",
    "    for start_node in range(num_points):\n",
    "        # print(start_node)\n",
    "        if not visited[start_node]:\n",
    "            cluster = []\n",
    "            stack = [start_node]\n",
    "\n",
    "            while stack:\n",
    "                node = stack.pop()\n",
    "                if not visited[node]:\n",
    "                    cluster.append(node)\n",
    "                    visited[node] = True\n",
    "                    for neighbor in range(num_points):\n",
    "                        if links_matrix[node][neighbor] == 1 and not visited[neighbor]:\n",
    "                            stack.append(neighbor)\n",
    "\n",
    "            clusters.append(cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# Read the data\n",
    "track_points_df = pd.read_csv(\"go_track_trackspoints.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "track_points_df = preprocess_data(track_points_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract zone data and convert it into the required format\n",
    "zone_data = track_points_df['zone'].tolist()\n",
    "\n",
    "# Implement the ROCK algorithm\n",
    "threshold_rock = 0.5  # Example threshold value for ROCK\n",
    "clusters_rock = ROCK(zone_data, threshold_rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the resulting clusters from ROCK\n",
    "print(\"ROCK Clusters:\")\n",
    "for i, cluster in enumerate(clusters_rock):\n",
    "    print(f\"Cluster {i+1}: {cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implementing the Chameleon Algorithm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Function to categorize latitude and longitude into zones\n",
    "def categorize_zone(latitude, longitude):\n",
    "    num_lat_zones = 10\n",
    "    num_lon_zones = 10\n",
    "    lat_zone_size = (max(latitude) - min(latitude)) / num_lat_zones\n",
    "    lon_zone_size = (max(longitude) - min(longitude)) / num_lon_zones\n",
    "    zones = []\n",
    "    for lat, lon in zip(latitude, longitude):\n",
    "        lat_zone = int((lat - min(latitude)) // lat_zone_size)\n",
    "        lon_zone = int((lon - min(longitude)) // lon_zone_size)\n",
    "        zones.append((lat_zone, lon_zone))\n",
    "    return zones\n",
    "\n",
    "# Function to handle missing values and preprocess the data\n",
    "def preprocess_data(track_points_df):\n",
    "    track_points_df.dropna(inplace=True)\n",
    "    track_points_df['zone'] = categorize_zone(track_points_df['latitude'], track_points_df['longitude'])\n",
    "    return track_points_df\n",
    "\n",
    "# Function for graph partitioning\n",
    "def graph_partitioning(data, k):\n",
    "    G = nx.Graph()\n",
    "    for i, point in enumerate(data):\n",
    "        print(f\"Processing point {i}/{len(data)}\")\n",
    "        distances = [(j, euclidean(point, data[j])) for j in range(len(data)) if i != j]\n",
    "        k_nearest = sorted(distances, key=lambda x: x[1])[:k]\n",
    "        for neighbor, distance in k_nearest:\n",
    "            G.add_edge(i, neighbor, weight=distance)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for agglomerative hierarchical clustering\n",
    "def agglomerative_clustering(G, data):\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    return clusters\n",
    "\n",
    "# Function to compute the relative closeness\n",
    "def relative_closeness(cluster_i, cluster_j, data):\n",
    "    centroid_i = np.mean(data[cluster_i], axis=0)\n",
    "    centroid_j = np.mean(data[cluster_j], axis=0)\n",
    "    return 1 / (1 + euclidean(centroid_i, centroid_j))\n",
    "\n",
    "# Function to compute the relative interconnectivity\n",
    "def relative_interconnectivity(cluster_i, cluster_j, G):\n",
    "    num_edges_within = G.subgraph(cluster_i).number_of_edges() + G.subgraph(cluster_j).number_of_edges()\n",
    "    num_edges_between = nx.number_of_edges(G) - num_edges_within\n",
    "    if num_edges_between == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num_edges_within / num_edges_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the merge cost\n",
    "def merge_cost(cluster_i, cluster_j, data, G):\n",
    "    closeness = relative_closeness(cluster_i, cluster_j, data)\n",
    "    interconnectivity = relative_interconnectivity(cluster_i, cluster_j, G)\n",
    "    return closeness * interconnectivity\n",
    "\n",
    "# Function to merge clusters based on the merge cost\n",
    "def merge_clusters(clusters, data, G):\n",
    "    min_cost = float('inf')\n",
    "    best_merge = None\n",
    "    for i, cluster_i in enumerate(clusters):\n",
    "        for j, cluster_j in enumerate(clusters[i+1:]):\n",
    "            cost = merge_cost(cluster_i, cluster_j, data, G)\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                best_merge = (i, j+i+1)\n",
    "    if best_merge is not None:\n",
    "        i, j = best_merge\n",
    "        clusters[i].extend(clusters[j])\n",
    "        del clusters[j]\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "track_points_df = pd.read_csv(\"go_track_trackspoints.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "track_points_df = preprocess_data(track_points_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Chameleon algorithm\n",
    "k = 10  # Number of neighbors for graph partitioning\n",
    "G = graph_partitioning(track_points_df['zone'], k)\n",
    "clusters_chameleon = agglomerative_clustering(G, track_points_df['zone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the resulting clusters from Chameleon algorithm\n",
    "print(\"\\nChameleon Clusters:\")\n",
    "for i, cluster in enumerate(clusters_chameleon):\n",
    "    print(f\"Cluster {i+1}: {cluster}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
